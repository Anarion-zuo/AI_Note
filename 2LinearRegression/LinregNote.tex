\documentclass{article}
\usepackage{geometry}
\usepackage{amsmath}

\begin{document}
\section{Linear Regression}
\subsection{Residual and Regular}
	\subsubsection{Residual}
	For given data set $x$ and $y$, we try to find the relation between them therefore make use of our research, which means:
	\begin{displaymath}
		D=\{x_i, y_i\}^N_{i=1}\qquad find \quad y = f(x)
	\end{displaymath}
	For the simplest case, which is linear regression, we want to find vector w, so that,
	\begin{displaymath}
		y = w^Tx=\begin{pmatrix}
		w_0 & w_1 & ... & w_D
		\end{pmatrix}
		\begin{pmatrix}
		1 \\ x_1 \\ x_2 \\ ... \\ x_D
		\end{pmatrix}
		= w_0 + \sum_{j=1}^{D}w_jx_j
	\end{displaymath}
	where vector $w$ is the goal of the process, known as Regression Coefficient, while D stands for the number of dimensions. Note that $w_0$, the Bias, is specifically listed out so that the constant term is not ignored.
	The Goal, however, is to let the error of the approximation be as little as possible, although we cannot be absolutely sure how close is our model to the real world. Anyway, it is still a practicle model if the real world does not change rapidly and randomly, and if the error of the model under known data set is minimized. Therefore, we have the method of minimizing ERROR. We take the ERROR simply to be the difference between the model and the real data.
	\begin{displaymath}
		r=y-\hat{y}
	\end{displaymath}
	where $r$ stands for residual, $y$ stands for real data, $\hat{y}$ stands for estimated data. They are all vectors. Note that in order to prevent the positive and negative value from canceling each other, normally we take 2 kinds of adjustment.
	\begin{displaymath}
		r_{L_1} = |y - \hat{y}|\qquad r_{L_2} = (y - \hat{y})^2
	\end{displaymath}
	There, we want the sum of the squared value of the residual to be minimized.
	\begin{displaymath}
		RSS = \sum_{i=1}^{N}r_i
	\end{displaymath}
	And we take the $r_i$ that seems fit to us.
	\subsubsection{Probability Explaination of Residual}
	Here, we are trying to find out why is it the right thing to do that we try to minimize the residual.
	\paragraph{L2 Case}
	Take:
	\begin{displaymath}
		RSS(w) = \sum_{i=1}^{N}L(y_i,\hat{y_i}) = \sum_{i=1}^{N}(y_i - f(x_i))^2
	\end{displaymath}
	If the data has a certain noise:
	\begin{displaymath}
		\epsilon \sim N(0,\sigma ^2) \quad y = f(x) + \epsilon \qquad y|_{x = x_i} \sim N(f(x_i), \sigma ^2)
	\end{displaymath}
	Maximum likelihood estimation:
	\begin{displaymath}
		\ln(\prod_{i=1}^{N}p_{y_i|x_i}(y|x=x_i) )
		= -\frac{N}{2}\ln(2\pi) - N\ln\sigma - \sum_{i=1}^{N}\frac{(y_i-f(x_i))^2}{2\sigma ^2}
	\end{displaymath}
	As we can see in the function above, to take the maximun of RHS is to take the minimun of the L2 residual of the model. Therefore, these two are equivalent.
	\paragraph{L1 Case}
	Take:
	\begin{displaymath}
	RSS(w) = \sum_{i=1}^{N}L(y_i,\hat{y_i}) = \sum_{i=1}^{N}|y_i - f(x_i)|
	\end{displaymath}
	If the data has a certain noise:
	\begin{displaymath}
	\epsilon \sim Laplace(0,b) \quad y = f(x) + \epsilon \qquad y_{i|x=x_i} \sim Laplace(f(x_i),b)
	\sim p_{y|x=x_i}(y|x=x_i) = \frac{1}{2b} e^{-\frac{|y-f(x_i)|}{b}}
	\end{displaymath}
	Maximum likelihood estimation:
	\begin{displaymath}
	\ln(\prod_{i=1}^{N}p_{y_i|x_i}(y|x=x_i))
	= -\ln(2b) - \sum_{i=1}^{N}\frac{|y_i-f(x_i)|}{b}
	\end{displaymath}
	\paragraph{Differeces between L1 and L2}
	L1 method is not sensitive to noise while L2 method is very sensitive to noise, which means noise data has a larger weight in L2 model. However, L1 function has a major disadvantage, which is its  incontinuity, therefore finding its minimun value can be hard. In order to combine the ups and eliminate the downs of the two models, we have the Huber method.
	\begin{displaymath}
	L_\delta(r) = \left\{ \begin{array}{ll}
	\frac{1}{2}r^2 & |r| \le \delta \\
	\delta |r| - \frac{1}{2}\delta ^2 & others
	\end{array} \right.
	\end{displaymath}
	\subsubsection{Over Regression}
	For given data set, we tend to fit our model into the data set as much as possible. Thus in some cases, we manage to let the model fit every bit of the data set, or nearly achieve that. It is not hard for us to do so, for the simpliest method of polynomial regression. If we have a data set with the amount of $n$, we simply need to construct a model holding $n$th power. However, even if the model seems to fit well enough, under the real circumstance, it will not fit, for every data set has its particularity and we should not try to maximize that in our model. Therefore, we want to hold the training error to be small and also hold the testing error to be small. The case of training error is very small and testing error is large is called Over Regression, meaning that the model is too fit for the training data thus too unfit for the real world.
	\subsubsection{Regular Term}
	In order to prevent over regression from happening, we have the method of adding a regular term. $R(w)$ is the Regular Term.
	\paragraph{L2 Loss + L2 Regular: Ridge}
	\begin{displaymath}
		J(w,\lambda) = \sum_{i=0}^{N} r^2 + \lambda \sum_{j=1}^{N} w_j^2 \qquad where\quad R(w) = \lambda \sum_{j=1}^{N} w_j^2
	\end{displaymath}
	The process of constructing the model becomes finding the minimun value of $J$. Note that we do not add punishment AKA regulation upon the Bias term and $j$ starts at 1. It is important for us to choose an appropriate $\lambda$. It is better to nondimensionalize the data set upon all dimensions. Same thing holds for the others.
	\paragraph{L2 Loss + L1 Regular: Lasso}
	\begin{displaymath}
		R(w) = \sum_{j=1}^{N}|w_j|
	\end{displaymath}
	\paragraph{L2 Loss + Both Regular: Elastic Net}
	\begin{displaymath}
		R(w) = \sum_{j=1}^{N}(\rho |w_j| + \frac{1 - \rho}{2}w_j^2)
	\end{displaymath}
	\subsubsection{Probability Explaination of Regular}
	\subsection{Analytic Solution of OLS}
	\paragraph{OLS Formular}
	\begin{displaymath}
		J(w) = \sum_{i=1}^{N}(y_i - f(x_i))^2
	\end{displaymath}
	\paragraph{Goal}
	\begin{displaymath}
		\hat{w} = argmin_w J(w)
	\end{displaymath}
	\paragraph{Requirement}
	\begin{displaymath}
		\frac{\partial J(w)}{\partial w} = 0
	\end{displaymath}
	\paragraph{Normal Equation}
	\begin{gather*}
	J(w) = ||y - Xw||^2_2 = (y - Xw)^T(y - Xw) = y^Ty - y^TXw - w^TX^Ty + w^TX^TXw\\
	\frac{\partial J(w)}{\partial w} = -2X^Ty + 2X^TXw = 0 \qquad X^TXw = X^Ty\\
	\hat{w}_{OLS} = (X^TX)^{-1}X^Ty
	\end{gather*}
	\subsection{Gradient Descent Method(GD)}
	\paragraph{Why do We Need It?}
	The analytical method takes running time of $O(N^2D)$, for matrix $M\times D$. Sometimes it is too slow to use the analytical method, as well as the memory of the computer might not be enough for a single set of data.
	\paragraph{How Do We Do It?}
	The Gradient Descent Method is simply to calculate the gradient of the function at a initial point so that the fastest descending direction of the function can be found. Then let the function go towards that direction and take the gradient again, so and so forth. We continue iterating the process until the function value is close enough to the last result.

	Normally, we take the minmun value of the function, and if we want a max of the function, we simply take the min of its negative version. Therefore, we have the following steps.
	\\\\
	fundamental equation:
	\begin{displaymath}
		x_{t+1} = x + \Delta x = x_{t} - \eta \nabla f(x)\qquad for\quad \eta \ge 0
	\end{displaymath}
	\paragraph{under the OLS case:}
	\begin{displaymath}
		J(w) = \sum_{i=1}^{n}(y_i-w^Tx_i)^2 = ||y - Xw||^2_2 = (y-Xw)^T(y-Xw)
	\end{displaymath}
	gradient:\begin{displaymath}
		\nabla J(w) = -2X^Ty + 2X^TXw = -2X^T(y - Xw)
	\end{displaymath}
	plug into the iteration:\begin{displaymath}
		w_{t+1} = w_t - \eta \nabla J(w_t) = w_t + 2\eta X^T(y - Xw_t)
	\end{displaymath}
	in which $(y - Xw_t)$ is the residua of the prediction.

	With the equations above, we substitude 0 or a small random data set into the equations as initial values, then iteratively calculate the $w$ according to a certain learning rate $\eta$, until it satisfies the breaking condition. The breaking condition can be:

	1. Iterating time meets the maximun value;

	2.\begin{equation}
		\frac{J(w_t)-J(w_{t+1})}{J(w_t)} \le \epsilon;
	\end{equation}

	There, we have the full process of simple Gradient Discent Method. Mind that the choice of the learning rate is important. If learning rate is too large, in the end, there might be oscillation in the result.
	\paragraph{Ridge Regression}
	\begin{gather*}
		J(w) = ||y - Xw||^2_2 + \lambda ||w||^2_2\\
		\nabla J(w) = -2X^Ty + 2X^TXw + 2\lambda w
	\end{gather*}
	\paragraph{Stochastic Gradient Descent(SGD)}
	When the sample is too complex or the gradients upon all dimensions are too alike, it is not so appropriate that we still apply the simple GD method to the sample. Therefore, we now apply the Stochastic GD method.We simply take index t by some approach, usually randomly.
	\begin{gather*}
		\nabla J(w_t) = \nabla L(y_t, f(x_t;w_t)) + \lambda \nabla R(w_t)
	\end{gather*}
\subsection{Coordinate Descent Method}
	\subsubsection{Subderivative}
	In some cases like Lasso, when absolute value is involved and the function is not continuous at some points, the original concept of derivative and maximizing function is not valid. At the incontinuous point, there could be many lines which satisfy the feature of tangency, which is that around the certain point, the line does not cross the function as well as has a single intersection point with the function. There can be a group of lines here, whose slope make up the set of the subderivative. According to the defination, the subderivative is noted as $\partial f(x_0)$, also:
	\begin{displaymath}
		\partial f(x_0) \in
		[a,b]\qquad where\quad
		a = \lim_{x \rightarrow x_0^-} \frac{f(x)-f(x_0)}{x-x_0},
		b = \lim_{x \rightarrow x_0^+} \frac{f(x)-f(x_0)}{x-x_0}
	\end{displaymath}
	The upper and lower bound of subderivative is determined by the left and right limit of the derivative. For example, take $f(x) = |x|$.
	\begin{displaymath}
		\partial f(x) = \left\{\begin{array}{ll}
			-1 & \textrm{(x < 0)}\\
			\left[-1,1\right] & \textrm{(x = 0)}\\
			1 & \textrm{(x > 0)}
			\end{array} \right.
	\end{displaymath}
	Obviously, if the function is continuous, the subderivative set contains only its derivative. Similarly, we define Subgradient for multivariable functions. If 0 is included in the Subgradient set, we have the critical point.

	There, we can apply the gradient descent method to incontinuous functions, changing the ending condition into the set is small enough and includes 0. The gradient here is not always descending, therefore, we take all of the calulated $f(x_{t})$ and take their minimun.
	\subsubsection{Coordinate Descent Method}
	Instead of taking all of the dimensions into consideration at the same time, this time, we only take one dimension each time and find the minimun value.
	Here, we take the example of Lasso.
	\begin{displaymath}
		J(w) = ||y - Xw||^2_2 + \lambda ||w||_1
	\end{displaymath}
	For the jth dimension:
	\begin{displaymath}\begin{aligned}
		\frac{\partial}{\partial w_j} RSS(w)
		&= \frac{\partial}{\partial w_j} \sum_{i=1}^{N}(y_i - (w_{-j}^Tx_{i,-j} + w_jx_{ij}))^2\\
		&= -2\sum_{i=1}^N (y_i - w^T_{-j}x_{i,-j} - w_ix_{ij})x_{ij}\\
		&= 2\sum_{i=1}^N x_{ij}^2w_j - 2\sum_{i=1}^N (y_i - w^T_{-j}x_{i,-j)})x_{ij}\\
		&=a_jw_j - c_j
	\end{aligned}
	\end{displaymath}
\subsection{Evaluation}
	\subsubsection{Rooted Mean Square Error(RMSE)}
	\begin{displaymath}
		RMSE(y,\hat{y}) = \sqrt{\frac{1}{N} \sum_{i=1}^{N}(y_i-\hat{y}_i)^2}
	\end{displaymath}
	\subsubsection{Mean Absolute Error(MAE)}
	\begin{displaymath}
		MAE(y,\hat{y}) = \frac{1}{N} \sum_{i=1}^{N}|y_i-\hat{y}_i|
	\end{displaymath}
	\subsubsection{Median Absolute Error(MedAE)}
	\begin{displaymath}
		MedAE(y,\hat{y}) = median(|y_1-\hat{y}_1|,...,|y_N-\hat{y}_N|)
	\end{displaymath}
	\subsubsection{Mean Squared Logarithmic Error(MSLE)}
	\begin{displaymath}
		MSLE(y,\hat{y}) = \frac{1}{N} \sum_{i=1}^{N}{\ln(1+y_i) - \ln(1+\hat{y}_i)^2}
	\end{displaymath}
	\subsubsection{$R^2$ Score}
	\begin{gather*}
		SS_{res}(y,\hat{y}) = \frac{1}{N} \sum_{i=1}^{N}(y_i-\hat{y}_i)^2,SS_{tot}(y) = \frac{1}{N}\sum_{i=1}^{N}(y_i - \bar{y})^2\\
		R^2(y,\hat{y}) = 1 - \frac{SS_{res}(y,\hat{y})}{SS_{tot}(y)}
	\end{gather*}
	\subsubsection{Methods}
	Ideally, it can be better if we have a certain data sample to build a model and have extra data sample to examine it. However, we only have one data sample in the real world, therefore we have to divide it into training data and testing data. Here, we introduce a method called Cross Validation. Simply, we divide the sample into N parts and try N times with each time having a different part as the testing data and the rest as training data.
	\begin{gather*}
		\mbox{For the $k$th time}:e_k = e(y_{test_k},f(X_{test\_k};\hat{w}_k;\lambda))\\
		\mbox{Gather the result for certain $\lambda$}:e_\lambda = \frac{1}{K} \sum_{k=1}^{K}e_k
	\end{gather*}
	Furthermore, it is better that we try different $\lambda$ and get a fitter result.
	\paragraph{Bootstrap}
	For the Cross Validation mentioned above, we are taking one peice of data sample each time and make sure the same data will not be taken again. Here, we change our way of taking subsamples and take randomly $\frac{1}{N}$ of the sample each time, regardless of whether the data has been taken before. Therefore, after N times of taking subsamples, the probability of a certain peice of data having not been taken for a single time is $(1-\frac{1}{N})^N$, which goes to 0.368, which means only around $63.2\%$ of the sample is taken with Bootstrap.
\subsection{Exploratory Data Analysis(EDA)}
\subsubsection{Basic Information}
By the time we start, there are some basic things we need to figure out.
\begin{enumerate}
	\item Dimension of the sample
	\item Amount of the sample
	\item Physical meaning of each dimension
	\item Type of each dimension, categorical features or discrete features or numeric feature
\end{enumerate}
\subsubsection{Behavior of Each Dimension}
\begin{enumerate}
	\item Display basic statistical magnitude, mean, variance, quantiles.
	\item A glimpse of distributions of each dimension by a histogram. Most of the numeric features are usually normally distrubuted. If not, it might be that some data out of a certain range is counted into the lower and upper bound, thus both ends might behave unordinarily large, therefore might be dumped.
	\item A glimpse of correlations between dimensions. Build the correlation matrix with heat map. For the dimensions that are strongly linearly correlated perceived from the matrix, print out the scatter diagram of the two dimensions. 
\end{enumerate}
\subsection{Feature Engineering(FE)}

\subsubsection{Noise}
\subsubsection{Saperation}
\subsubsection{Encoding}
\subsubsection{Feature Pre-process}
\end{document}
